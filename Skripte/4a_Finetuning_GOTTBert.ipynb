{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"4YsHbwhawDSJ","colab":{"base_uri":"https://localhost:8080/","height":417},"executionInfo":{"status":"error","timestamp":1741030172546,"user_tz":-60,"elapsed":23250,"user":{"displayName":"Nele Johanna","userId":"12907834607396906617"}},"outputId":"01ff1a48-b464-4370-f180-a301cd21aadb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'datasets'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-958b3c933f9b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#!pip install datasets evaluate transformers[sentencepiece]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#Pfad individuell dorthin setzen, wo die Daten liegen                           ####\n","path = '/content/drive/MyDrive/techlabs/Github/Daten/'\n","\n","## Laden aller nötigen relevanten Pakete\n","import pandas as pd\n","import os\n","import torch\n","# diese Setzung wurde später beim Training empfohlen\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","\n","#!pip install datasets evaluate transformers[sentencepiece]\n","\n","import datasets\n","from datasets import load_dataset\n","from transformers import  AutoModelForSequenceClassification, get_scheduler\n","\n","#!pip install accelerate\n","#from accelerate import Accelerator\n"]},{"cell_type":"code","source":["# Welche Variante soll trainiert werden? Die mit oder die ohne Partei?          #### !!\n","with_party_info = [\"no\", \"yes\"][1]\n","with_party_info\n","\n","# Laden der Datensätze und auch schon Definition des späteren Modellnamens\n","if with_party_info == \"yes\":\n","  train_df = pd.read_parquet(path + \"df_train2.parquet\")\n","  val_df = pd.read_parquet(path + \"df_val.parquet\")\n","  #test_df = pd.read_parquet(path + \"df_test.parquet\")\n","\n","  model_name = \"GottBERT_model\"\n","  #model_name = \"second_GottBERT_model\"\n","  #previous_model_name = \"GottBERT_model\"\n","\n","else:\n","  train_df = pd.read_parquet(path + \"df_train2_np.parquet\")\n","  val_df = pd.read_parquet(path + \"df_val_np.parquet\")\n","  #test_df = pd.read_parquet(path + \"df_test_np.parquet\")\n","\n","  model_name =  \"GottBERT_model_np\"\n","  #model_name = \"second_GottBERT_model_np\"\n","  #previous_model_name = \"GottBERT_model_np\"\n"],"metadata":{"id":"yr4ZF-4YwNnU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Überprüfen, ob es geklappt hat\n","print(train_df.head())\n","print(train_df.tail())\n","print(val_df.tail())"],"metadata":{"collapsed":true,"id":"z2VqndhuUNwo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Überführe die Datensätze in die für das Training richtige Dateiformat\n","from datasets import Dataset\n","from datasets import DatasetDict\n","\n","ds_training = Dataset.from_pandas(train_df[['Rede', 'labels']])\n","ds_val = Dataset.from_pandas(val_df[['Rede', 'labels']])\n","# ds_test = Dataset.from_pandas(df_test[['Rede', 'labels']])\n","\n","dataset = DatasetDict({\n","    'train': ds_training,\n","    'validation': ds_val\n","    })"],"metadata":{"id":"ntNdwFbmWTxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BNb36EEhfUy","collapsed":true},"outputs":[],"source":["# Importiere das Modell\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","\n","checkpoint = 'TUM/GottBERT_base_best'   #\"deepset/gbert-base\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"]},{"cell_type":"code","source":["#probiere den Tokenizer an einem Satz aus\n","test = dataset[\"train\"][1][\"Rede\"]\n","inputs = tokenizer(test)\n","tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"],"metadata":{"collapsed":true,"id":"nBqkdG-Qcqn7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Definiere eine Funktion zum Tokenizieren und wende diese auf das dataset an\n","\n","def tokenize_function(example):\n","  return tokenizer(example[\"Rede\"],truncation=True, padding = \"max_length\", max_length = 512)\n","\n","tokenized_dataset = dataset.map(tokenize_function, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"],"metadata":{"id":"1-2UcshJxsjI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#entferne irrelevante Spalten\n","tokenized_dataset = tokenized_dataset.remove_columns(['Rede', '__index_level_0__'])\n","\n","#set the format of the datasets so they return PyTorch tensors instead of lists\n","tokenized_dataset.set_format(\"torch\")\n","\n","#Lade die Daten in den DataLoader\n","from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(\n","    tokenized_dataset[\"train\"], shuffle=True, batch_size= 40, collate_fn=data_collator\n",")\n","eval_dataloader = DataLoader(\n","    tokenized_dataset[\"validation\"], batch_size= 40, collate_fn=data_collator\n",")"],"metadata":{"id":"FQg1_sfxavcZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Wichtiger Zwischenschritt: Irrelevante Datensätze löschen, damit RAM frei wird\n","import gc\n","del train_df\n","del val_df\n","del ds_val\n","del ds_training\n","del dataset\n","\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z9_iLx-23xrV","outputId":"bd464acd-f0df-465d-a170-aea996f9e1c4","executionInfo":{"status":"ok","timestamp":1741006647856,"user_tz":-60,"elapsed":5,"user":{"displayName":"Nele Johanna","userId":"12907834607396906617"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["62"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["# Überprüfen des Formats\n","for batch in train_dataloader:\n","    break\n","{k: v.shape for k, v in batch.items()}"],"metadata":{"id":"tvm_9RalccnO","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"outputId":"68a7f588-032f-4939-c229-2cdc9575fad9","executionInfo":{"status":"ok","timestamp":1741006649594,"user_tz":-60,"elapsed":25,"user":{"displayName":"Nele Johanna","userId":"12907834607396906617"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'labels': torch.Size([40]),\n"," 'input_ids': torch.Size([40, 512]),\n"," 'attention_mask': torch.Size([40, 512])}"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["# Lade GottBERT-Modell mit der Aufgabe Sequenz-Klassifikation\n","from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=6)\n","\n","# ggf. (bei sequentiellem Trainieren): Lade Gewichte des votrainierten Modells\n","#model.load_state_dict(torch.load(path + previous_model_name + \".pth\"))\n","\n","#outputs = model(**batch)\n","#print(outputs.loss, outputs.logits.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ggIijdkw4iZF","outputId":"9d9349ea-f31f-449e-d1db-45978a77563a","executionInfo":{"status":"ok","timestamp":1741006652399,"user_tz":-60,"elapsed":112,"user":{"displayName":"Nele Johanna","userId":"12907834607396906617"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at TUM/GottBERT_base_best and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFEXV-p2YHLa","collapsed":true},"outputs":[],"source":["# Entscheide über Hyper-Parameter zum Trainieren\n","from torch.optim import AdamW\n","\n","optimizer = AdamW(model.parameters(), lr= 5e-5)\n","\n","from transformers import get_scheduler\n","\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")\n","\n","print(num_training_steps)\n"]},{"cell_type":"code","source":["# Nutze die GPU\n","import torch\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","print(device)"],"metadata":{"id":"-50E6NXtUeZy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"148f8b30-21f0-4f58-fa75-0340a69a0582","executionInfo":{"status":"ok","timestamp":1741006663004,"user_tz":-60,"elapsed":210,"user":{"displayName":"Nele Johanna","userId":"12907834607396906617"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["#Nun trainiere das Modell!\n","from tqdm.auto import tqdm\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","       # daten zur GPU schicken\n","       batch = {k: v.to(device) for k, v in batch.items()}\n","       #compute model-outputs\n","       outputs = model(**batch)\n","       #compute loss\n","       loss = outputs.loss\n","       #compute gradients with loss\n","       loss.backward()\n","       #make a training step\n","       optimizer.step()\n","       #update learning rate\n","       lr_scheduler.step()\n","       #zero the gradients\n","       optimizer.zero_grad()\n","       progress_bar.update(1)\n","\n","# Speichere das trainierte Modell ab\n","torch.save(model.state_dict(), path + model_name + \".pth\")\n","\n","#so kann man es wieder laden:\n","#model.load_state_dict(torch.load(path + model_name + \".pth\"))"],"metadata":{"id":"caOFDX4Gd_Fa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Modell evaluieren\n","\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","from evaluate import load\n","\n","#Lade das Parteien-Mapping\n","import pickle\n","with open(path +'party_mapping.pkl', 'rb') as f:\n","    parties = pickle.load(f)\n","fraktionen_labels = [str(k) for k in parties]\n","\n","all_predictions = []\n","all_labels = []\n","\n","metric = load(\"glue\", \"mrpc\")\n","model.eval()\n","for batch in eval_dataloader:\n","  batch = {k: v.to(device) for k, v in batch.items()}\n","  with torch.no_grad():\n","    outputs = model(**batch)\n","\n","  logits = outputs.logits\n","  predictions = torch.argmax(logits, dim=-1)\n","  metric.add_batch(predictions = predictions, references = batch[\"labels\"])\n","\n","  all_predictions.extend(predictions.cpu().numpy())\n","  all_labels.extend(batch[\"labels\"].cpu().numpy())\n","\n","print(\"Accuracy:\",accuracy_score(all_labels, all_predictions))\n","print(\"f1-score:\", f1_score(all_labels, all_predictions, average = \"weighted\"))\n","\n","report = classification_report(all_labels, all_predictions)\n","print(\"\\nClassification Report:\")\n","print(report)\n","\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","\n","cm = confusion_matrix(all_labels, all_predictions)\n","disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = fraktionen_labels)\n","disp.plot()\n","plt.show()"],"metadata":{"id":"Kp3M65D89Qs_"},"execution_count":null,"outputs":[]}]}